<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="RecA: Reconstruction Alignment Improves Unified Multimodal Models">
  <meta property="og:title" content="RecA: Reconstruction Alignment Improves Unified Multimodal Models">
  <meta property="og:description" content="Introducing RecA, a self-supervised training framework that aligns understanding and generation through image reconstruction at the semantic level.">
  <meta property="og:url" content="https://horizonwind2004.github.io/RecA-Page">
  <meta name="twitter:title" content="RecA: Reconstruction Alignment Improves Unified Multimodal Models">
  <meta name="twitter:description" content="Introducing RecA, a self-supervised training framework that aligns understanding and generation through image reconstruction at the semantic level.">
  <meta name="keywords" content="RecA, Vision-Language, Multimodal, Image Generation, AI, Machine Learning">
  
  <title>RecA: Reconstruction Alignment Improves Unified Multimodal Models</title>
  
  <link rel="icon" type="image/png" href="static/files/teaser/logo.png">
  <link rel="shortcut icon" href="static/files/teaser/logo.png" type="image/png">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <script src="https://cdn.tailwindcss.com"></script>
  
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: {
            sans: ['Inter', 'sans-serif'],
          },
          colors: {
            primary: {
              50: '#f0f9ff',
              100: '#e0f2fe',
              200: '#bae6fd',
              300: '#7dd3fc',
              400: '#38bdf8',
              500: '#0ea5e9',
              600: '#0284c7',
              700: '#0369a1',
              800: '#075985',
              900: '#0c4a6e',
            },
            secondary: {
              50: '#f8fafc',
              100: '#f1f5f9',
              200: '#e2e8f0',
              300: '#cbd5e1',
              400: '#94a3b8',
              500: '#64748b',
              600: '#475569',
              700: '#334155',
              800: '#1e293b',
              900: '#0f172a',
            },
          },
        }
      }
    }
  </script>
  
  <style>
    .gradient-text {
      background: linear-gradient(90deg, #3b82f6, #8b5cf6);
      -webkit-background-clip: text;
      background-clip: text;
      color: transparent;
    }
    
    .hero-pattern {
      background-image: url('static/files/teaser/DEMO.jpg');
      background-size: cover;
      background-position: center;
      background-repeat: no-repeat;
      position: relative;
    }
    
    .hero-pattern::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background-color: rgba(0, 0, 0, 0.6);
    }
    
    .hero-content {
      position: relative;
      z-index: 10;
    }
    
    .card-hover {
      transition: all 0.3s ease;
    }
    
    .card-hover:hover {
      transform: translateY(-5px);
      box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
    }
    
    .animate-float {
      animation: float 6s ease-in-out infinite;
    }
    
    @keyframes float {
      0% { transform: translateY(0px); }
      50% { transform: translateY(-15px); }
      100% { transform: translateY(0px); }
    }
    
    .text-shadow {
      text-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
    }
    
    /* Custom styles for the demo gallery */
    .demo-gallery-container {
      position: relative;
    }
    
    .demo-gallery {
      display: flex;
      overflow-x: auto;
      scroll-snap-type: x mandatory;
      gap: 1rem;
      padding: 1rem 0;
      -webkit-overflow-scrolling: touch;
      scrollbar-width: none; /* Firefox */
    }
    
    .demo-gallery::-webkit-scrollbar {
      display: none; /* Chrome, Safari, Opera */
    }
    
    .demo-item {
      scroll-snap-align: start;
      flex: 0 0 auto;
      width: 350px;
      height: 350px;
      position: relative;
      border-radius: 0.5rem;
      overflow: hidden;
      box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
      transition: all 0.3s ease;
      cursor: pointer;
    }
    
    .demo-item:hover {
      transform: scale(1.05);
      z-index: 10;
    }
    
    .demo-item.active {
      border: 3px solid #3b82f6;
      transform: scale(1.02);
    }
    
    .demo-item img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    
    .demo-prompt-display {
      margin-top: 1rem;
      padding: 1rem;
      background: #f8fafc;
      border-radius: 0.5rem;
      border-left: 4px solid #3b82f6;
      min-height: 100px;
      display: flex;
      align-items: center;
      justify-content: center;
      text-align: center;
      font-size: 0.875rem;
      line-height: 1.4;
      color: #475569;
      opacity: 0;
      transition: opacity 0.3s ease;
    }
    
    .demo-prompt-display.active {
      opacity: 1;
    }
    
    .demo-prompt-display .prompt-text {
      font-weight: 500;
    }
    
    /* Edit comparison slider styles */
    .edit-comparison-container {
      position: relative;
      width: 100%;
      max-width: 600px;
      margin: 0 auto;
      border-radius: 0.75rem;
      overflow: hidden;
      box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
    }
    
    .edit-comparison-images {
      position: relative;
      width: 100%;
      height: 400px;
      overflow: hidden;
    }
    
    .edit-original, .edit-result {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
    
    .edit-original img, .edit-result img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    
    .edit-result {
      clip-path: polygon(50% 0%, 100% 0%, 100% 100%, 50% 100%);
    }
    
    .edit-slider {
      position: absolute;
      top: 0;
      left: 50%;
      width: 4px;
      height: 100%;
      background: white;
      cursor: ew-resize;
      transform: translateX(-50%);
      z-index: 10;
    }
    
    .edit-slider::before {
      content: '';
      position: absolute;
      top: 50%;
      left: 50%;
      width: 20px;
      height: 20px;
      background: white;
      border: 2px solid #3b82f6;
      border-radius: 50%;
      transform: translate(-50%, -50%);
    }
    
    .edit-labels {
      position: absolute;
      top: 1rem;
      left: 0;
      right: 0;
      display: flex;
      justify-content: space-between;
      padding: 0 1rem;
      z-index: 5;
    }
    
    .edit-label {
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 0.5rem 1rem;
      border-radius: 0.375rem;
      font-size: 0.875rem;
      font-weight: 500;
    }
    
    .edit-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 2rem;
    }
    
    .edit-prompt-area {
      margin-top: 1rem;
      padding: 1rem;
      background: #f8fafc;
      border-radius: 0.5rem;
      border-left: 4px solid #10b981;
      text-align: center;
      font-size: 0.875rem;
      color: #475569;
    }
    
    .edit-prompt-area .prompt-label {
      font-weight: 600;
      color: #10b981;
      margin-bottom: 0.5rem;
    }
    
    .placeholder-img {
      background: linear-gradient(135deg, #f3f4f6, #e5e7eb);
      display: flex;
      align-items: center;
      justify-content: center;
      color: #9ca3af;
      font-weight: 500;
    }
  </style>
</head>

<body class="font-sans bg-gray-50 text-gray-800">
  <!-- Navigation -->
  <nav class="bg-white/80 backdrop-blur-md shadow-sm sticky top-0 z-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="flex justify-between h-16">
        <div class="flex items-center">
          <img src="static/files/teaser/logo.png" alt="RecA Logo" class="h-8 w-8 mr-2">
          <span class="text-xl font-semibold text-gray-900">RecA</span>
        </div>
        <div class="hidden md:flex items-center space-x-8">
          <a href="#abstract" class="text-gray-700 hover:text-primary-600 transition">Abstract</a>
          <a href="#motivation" class="text-gray-700 hover:text-primary-600 transition">Motivation</a>
          <a href="#approach" class="text-gray-700 hover:text-primary-600 transition">Approach</a>
          <a href="#results" class="text-gray-700 hover:text-primary-600 transition">Results</a>
          <a href="#demo" class="text-gray-700 hover:text-primary-600 transition">Gallery</a>
          <a href="https://huggingface.co/spaces/sanaka87/BAGEL-RecA" target="_blank" class="inline-flex items-center px-4 py-2 border border-primary-600 text-sm font-medium rounded-md text-primary-600 bg-white hover:bg-primary-50 transition">
            <i class="fas fa-play-circle mr-2"></i>
            Demo
          </a>
        </div>
        <div class="flex items-center md:hidden">
          <!-- Mobile menu button -->
          <button class="text-gray-500 hover:text-gray-900 focus:outline-none">
            <svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
            </svg>
          </button>
        </div>
      </div>
    </div>
  </nav>

  <!-- Hero Section with Demo Background -->
  <section class="hero-pattern py-20">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 hero-content">
      <div class="text-center">
        <div class="flex justify-center items-center mb-6">
          <img src="static/files/teaser/logo.png" alt="RecA Logo" class="h-20 w-20 mr-4 animate-float">
          <h1 class="text-4xl md:text-5xl font-bold text-white text-shadow">
            <span class="gradient-text">RecA</span>: Reconstruction Alignment Improves Unified Multimodal Models
          </h1>
        </div>
        <h2 class="mt-4 text-xl md:text-2xl text-gray-200 max-w-4xl mx-auto text-shadow">
          Unlocking the Massive Zero-shot Potential in Unified Multimodal Models through Self-supervised Learning
        </h2>
        
        <!-- Authors -->
        <div class="mt-8 flex flex-wrap justify-center gap-6">
          <div class="bg-white/90 rounded-lg shadow-sm px-8 py-4 flex items-center backdrop-blur-sm">
            <div class="text-center">
              <p class="text-base font-medium text-gray-700">
                <a target="_blank" href="https://horizonwind2004.github.io/" class="text-primary-600 hover:text-primary-800 transition">Ji Xie</a><sup>1</sup>
              </p>
              <p class="text-sm text-gray-500">UC Berkeley</p>
            </div>
          </div>
          <div class="bg-white/90 rounded-lg shadow-sm px-8 py-4 flex items-center backdrop-blur-sm">
            <div class="text-center">
              <p class="text-base font-medium text-gray-700">
                <a target="_blank" href="https://people.eecs.berkeley.edu/~trevor/" class="text-primary-600 hover:text-primary-800 transition">Trevor Darrell</a><sup>1</sup>
              </p>
              <p class="text-sm text-gray-500">UC Berkeley</p>
            </div>
          </div>
          <div class="bg-white/90 rounded-lg shadow-sm px-8 py-4 flex items-center backdrop-blur-sm">
            <div class="text-center">
              <p class="text-base font-medium text-gray-700">
                <a target="_blank" href="https://homes.cs.washington.edu/~lsz/" class="text-primary-600 hover:text-primary-800 transition">Luke Zettlemoyer</a><sup>2</sup>
              </p>
              <p class="text-sm text-gray-500">University of Washington</p>
            </div>
          </div>
          <div class="bg-white/90 rounded-lg shadow-sm px-8 py-4 flex items-center backdrop-blur-sm">
            <div class="text-center">
              <p class="text-base font-medium text-gray-700">
                <a target="_blank" href="https://people.eecs.berkeley.edu/~xdwang/" class="text-primary-600 hover:text-primary-800 transition">XuDong Wang</a><sup>1*</sup>
              </p>
              <p class="text-sm text-gray-500">UC Berkeley</p>
            </div>
          </div>
        </div>
        
        <!-- Links -->
        <div class="mt-10 flex flex-wrap justify-center gap-4">
          <a class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-primary-600 cursor-not-allowed opacity-70">
            <i class="fas fa-file-pdf mr-2"></i> Coming soon
          </a>
          <a class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-gray-800 cursor-not-allowed opacity-70">
            <i class="fas fa-book-open mr-2"></i> Coming soon
          </a>
          <a class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-gray-800 cursor-not-allowed opacity-70">
            <i class="fab fa-github mr-2"></i> Coming soon
          </a>
          <a href="https://huggingface.co/collections/sanaka87/reca-68ad2176380355a3dcedc068" target="_blank" class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-yellow-600 hover:bg-yellow-700 transition">
            <i class="fas fa-robot mr-2"></i> HF Models
          </a>
          <a href="https://huggingface.co/spaces/sanaka87/BAGEL-RecA" target="_blank" class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-red-600 hover:bg-red-700 transition">
            <i class="fas fa-play-circle mr-2"></i> Demo (BAGEL)
          </a>
        </div>
      </div>
    </div>
  </section>

  <!-- Demo Section - Updated with Auto-scrolling Gallery -->
  <section id="demo" class="py-16 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900"> Gallery</h2>
        <p class="mt-4 text-lg text-gray-600 max-w-3xl mx-auto">
          Click on images to see their generation prompts below
        </p>
      </div>
      
      <div class="demo-gallery-container">
        <div class="demo-gallery" id="demoGallery">

          <div class="demo-item" data-prompt="A Van Gogh style painting of a cyberpunk city at sunrise.">
            <img src="static/files/demo/image1.jpg" alt="">
          </div>
          <div class="demo-item" data-prompt="A drop of water containing a miniature forest with colorful tiny flower inside.">
            <img src="static/files/demo/image2.png" alt="">
          </div>
          <div class="demo-item" data-prompt="A transparent glass cube on an endless desert, with a burning candle inside casting shadows on the sand.">
            <img src="static/files/demo/image3.png" alt="">
          </div>
          <div class="demo-item" data-prompt="A vibrant and traditional depiction of Tteokguk, a Korean rice cake soup, served during the Chuseok festival. The image features a steaming bowl of clear broth filled with soft, chewy rice cakes, slices of zucchini, carrots, and green onions, garnished with a sprinkle of sesame seeds. Surrounding the bowl are traditional Korean elements, such as a woven basket with red dates, a small wooden spoon, and a wooden table with a warm, earthy tone. The atmosphere is cozy and festive, with soft, natural lighting and a slightly blurred background to emphasize the dish. The scene captures the essence of Korean culture and the warmth of the Chuseok celebration, with a focus on authenticity and detail.">
            <img src="static/files/demo/image4.png" alt="">
          </div>
          <div class="demo-item" data-prompt="An anime-style portrait of a girl with big sparkling eyes, detailed hair highlights, soft gradient background, vibrant colors.">
            <img src="static/files/demo/image5.png" alt="">
          </div>
          <div class="demo-item" data-prompt="A dreamy composition of a young woman with butterflies emerging from her skin, wings glowing in soft golden hues, surreal and enchanting. She is wearing a wedding dress and has white angel wings, waving her hand.">
            <img src="static/files/demo/image14.png" alt="">
          </div>
          <div class="demo-item" data-prompt="A surreal split-face portrait: left side realistic woman with soft skin and a vivid blue eye, right side robotic cyborg with exposed steel plates, fluorescent blue circuits, tiny gears, and a blood-red mechanical eye, cinematic lighting, futuristic and striking.">
            <img src="static/files/demo/image15.png" alt="">
          </div>
          <div class="demo-item" data-prompt="Photorealistic closeup image of two pirate ships battling each other as they sail inside a cup of coffee.">
            <img src="static/files/demo/image16.png" alt="">
          </div>
          <div class="demo-item" data-prompt="A cute handmade felt doll of a little girl standing on a grassy patch. She wears an orange knitted hooded dress with blue buttons and matching boots. The girl is holding a smiling sun-shaped balloon made of felt. Fluffy white clouds with button details float in the sky, and soft green felt trees surround the scene. The style is whimsical, cozy, and playful, with pastel colors and a dreamy, handcrafted aesthetic.">
            <img src="static/files/demo/image17.png" alt="">
          </div>
          <div class="demo-item" data-prompt="The word 'RECA' is written on a street surface, with the word 'STARTS' written just below it, surrounded by colorful chalk drawings and playful doodles.">
            <img src="static/files/demo/image18.jpeg" alt="">
          </div>
          <div class="demo-item" data-prompt="A delicate blossom, a symbol of beauty and renewal in Japan, especially during the spring season.">
            <img src="static/files/demo/image21.png" alt="">
          </div>

        </div>
        
        <!-- Prompt Display Area -->
        <div class="demo-prompt-display" id="promptDisplay">
          <div class="prompt-text">Click on any image above to see its generation prompt</div>
        </div>
      </div>
      
      <!-- <div class="mt-8 text-center">
        <a href="https://huggingface.co/spaces/sanaka87/BAGEL-RecA" target="_blank" class="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-red-600 hover:bg-red-700 transition">
          <i class="fas fa-play-circle mr-2"></i> Try the Live Demo
        </a>
      </div> -->
    </div>
  </section>

  <!-- Edit Demo Section -->
  <section class="py-16 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">Interactive Editing Results</h2>
        <p class="mt-4 text-lg text-gray-600 max-w-3xl mx-auto">
          Drag the slider to compare original images with their edited versions
        </p>
      </div>
      
      <div class="edit-grid">
        <!-- Edit Demo 1 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="1">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori1.jpg" alt="Original Image 1">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit1.png" alt="Edited Image 1">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">Shed neon light on the scene.</div>
          </div>
        </div>

        <!-- Edit Demo 2 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="2">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori2.png" alt="Original Image 2">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit2.png" alt="Edited Image 2">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">Turn on the flashlight on the smartphone.</div>
          </div>
        </div>

        <!-- Edit Demo 3 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="3">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori3.png" alt="Original Image 3">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit3.png" alt="Edited Image 3">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">Remove the feather on the hat.</div>
          </div>
        </div>

        <!-- Edit Demo 4 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="4">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori4.png" alt="Original Image 4">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit4.png" alt="Edited Image 4">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">A man is wearing the clothes from the reference image.</div>
          </div>
        </div>

        <!-- Edit Demo 5 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="5">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori5.png" alt="Original Image 5">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit5.png" alt="Edited Image 5">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">Add strawberries, blueberries, and banana slices on top of the pancake stack.</div>
          </div>
        </div>

        <!-- Edit Demo 6 -->
        <div class="bg-white rounded-xl shadow-md p-6">
          <div class="edit-comparison-container" data-edit-id="6">
            <div class="edit-comparison-images">
              <div class="edit-original">
                <img src="static/files/edit_demo/ori6.png" alt="Original Image 6">
              </div>
              <div class="edit-result">
                <img src="static/files/edit_demo/edit6.png" alt="Edited Image 6">
              </div>
              <div class="edit-slider"></div>
              <div class="edit-labels">
                <div class="edit-label">Original</div>
                <div class="edit-label">Edited</div>
              </div>
            </div>
          </div>
          <div class="edit-prompt-area">
            <div class="prompt-label">Edit Prompt:</div>
            <div class="prompt-text">Change the gingerbread cookies into soft plush toy figures, with fuzzy fabric texture.</div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract Section -->
  <section id="abstract" class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">Abstract</h2>
        <div class="mt-4 h-1 w-20 bg-primary-600 mx-auto"></div>
      </div>
      
      <div class="bg-white rounded-xl shadow-md p-8 max-w-5xl mx-auto">
        <div class="prose-lg text-gray-700">
          <p class="mb-6">
            Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image–text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details—even when they use hundreds of words to describe a simple image.
          </p>
          <p class="mb-6">
            We introduce <span class="font-bold text-primary-600">Reconstruction Alignment</span> (<span class="font-bold text-primary-600">RecA</span>), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation.
          </p>
          <p class="mb-6">
            Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only <span class="font-bold text-primary-600">27 GPU-hours</span>, post-training with RecA substantially improves image generation performance on GenEval (<span class="font-bold text-primary-600">0.73→0.90</span>) and DPGBench (<span class="font-bold text-primary-600">80.93→88.15</span>), while also boosting editing benchmarks (ImgEdit <span class="font-bold text-primary-600">3.38→3.75</span>, GEdit <span class="font-bold text-primary-600">6.94→7.25</span>).
          </p>
          <p>
            Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Motivation Section -->
  <section id="motivation" class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">Motivation: What's Wrong With Text Supervision?</h2>
        <div class="mt-4 h-1 w-20 bg-primary-600 mx-auto"></div>
      </div>

      <!-- Motivation Figure -->
      <div class="bg-gray-50 rounded-xl shadow-md p-8 max-w-6xl mx-auto mb-12">
        <img src="static/files/teaser/motivation.jpg" alt="Dense supervision from visual embeddings vs. sparse text captions" class="w-full h-auto rounded-lg">
        <div class="text-center mt-4">
          <p class="text-sm text-gray-600 font-medium">
            <strong>Dense supervision from visual embeddings.</strong> 
            (a) Typical image generation models are trained on image-caption pairs where text provides only sparse supervision. 
            (b) By contrast, embeddings from visual understanding encoders preserve richer and more faithful semantics.
          </p>
        </div>
      </div>

      <div class="grid lg:grid-cols-2 gap-12 max-w-6xl mx-auto">
        <!-- The Problem: Sparse Text Supervision -->
        <div class="bg-gray-50 rounded-xl shadow-md p-8">
          <div class="flex items-start">
            <div class="flex-shrink-0">
              <div class="w-12 h-12 bg-red-100 rounded-lg flex items-center justify-center">
                <i class="fas fa-exclamation-triangle text-red-600 text-xl"></i>
              </div>
            </div>
            <div class="ml-6">
              <h3 class="text-xl font-semibold text-gray-800 mb-4">The Challenge: Sparse Text Supervision</h3>
              <div class="space-y-4 text-gray-700">
                <p>
                  Conventional training of Unified Multimodal Models (UMMs) relies on <span class="font-medium text-red-600">image-text pairs</span>, where captions provide supervision. However, even captions spanning hundreds of words omit critical visual details:
                </p>
                <ul class="list-disc list-inside space-y-2 text-sm">
                  <li><span class="font-medium">Spatial layout</span> and geometric structure</li>
                  <li><span class="font-medium">Fine-grained attributes</span> like textures and styles</li>
                  <li><span class="font-medium">Precise object shapes</span> and orientations</li>
                  <li><span class="font-medium">Exact color distributions</span> and lighting</li>
                </ul>
                <div class="bg-red-50 border-l-4 border-red-400 p-4 mt-6">
                  <div class="flex">
                    <div class="flex-shrink-0">
                      <i class="fas fa-quote-left text-red-400"></i>
                    </div>
                    <div class="ml-3">
                      <p class="text-sm italic text-red-700">
                        "An image is worth far more than a hundred words" — even lengthy captions miss key aspects, leading to biased correlations like <em>broccoli → green</em>.
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- The Solution: Dense Visual Embeddings -->
        <div class="bg-gray-50 rounded-xl shadow-md p-8">
          <div class="flex items-start">
            <div class="flex-shrink-0">
              <div class="w-12 h-12 bg-green-100 rounded-lg flex items-center justify-center">
                <i class="fas fa-lightbulb text-green-600 text-xl"></i>
              </div>
            </div>
            <div class="ml-6">
              <h3 class="text-xl font-semibold text-gray-800 mb-4">Our Solution: Dense Visual Supervision</h3>
              <div class="space-y-4 text-gray-700">
                <p>
                  Instead of relying on sparse text captions, we leverage <span class="font-medium text-green-600">visual understanding encoder embeddings</span> that map pixels into a language-aligned semantic space:
                </p>
                <ul class="list-disc list-inside space-y-2 text-sm">
                  <li><span class="font-medium">Semantic embeddings</span> from CLIP, SigLIP preserve richer details</li>
                  <li><span class="font-medium">Dense supervision</span> without paired captions</li>
                  <li><span class="font-medium">Language-aligned</span> and interpretable by UMMs</li>
                  <li><span class="font-medium">Captures layout, color, attributes</span> beyond text descriptions</li>
                </ul>
                <div class="bg-green-50 border-l-4 border-green-400 p-4 mt-6">
                  <div class="flex">
                    <div class="flex-shrink-0">
                      <i class="fas fa-question-circle text-green-400"></i>
                    </div>
                    <div class="ml-3">
                      <p class="text-sm italic text-green-700">
                        <em>Can we improve generation capabilities by training UMMs with semantic embeddings as maximally informative "visual prompts"?</em>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- The Misalignment Problem -->
      <div class="mt-12 bg-yellow-50 rounded-xl shadow-md p-8 max-w-5xl mx-auto">
        <div class="text-center mb-6">
          <h3 class="text-2xl font-semibold text-gray-800">Understanding vs. Generation Misalignment</h3>
        </div>
        
        <!-- Misalignment Example Image -->
        <div class="bg-white rounded-lg shadow-sm p-6 mb-8 max-w-3xl mx-auto">
          <img src="static/files/teaser/misalignment.jpg" alt="Yellow broccoli misalignment example" class="w-full h-auto rounded-lg">
          <div class="text-center mt-4">
            <p class="text-sm text-gray-600">
              <strong>Example:</strong> UMMs can often correctly recognize an uncommon concept (yellow broccoli) but fail to generate it, revealing misalignment between understanding and generation.
            </p>
          </div>
        </div>
        
        <div class="flex flex-col md:flex-row items-center gap-8">
          <div class="flex-1">
            <div class="bg-white rounded-lg p-6 shadow-sm">
              <div class="text-center">
                <div class="w-20 h-20 mx-auto bg-blue-100 rounded-full flex items-center justify-center mb-4">
                  <i class="fas fa-eye text-blue-600 text-2xl"></i>
                </div>
                <h4 class="font-semibold text-gray-800 mb-2">Understanding</h4>
                <p class="text-sm text-gray-600">✅ Can recognize "yellow broccoli"</p>
              </div>
            </div>
          </div>
          <div class="flex-shrink-0">
            <div class="w-16 h-16 bg-red-100 rounded-full flex items-center justify-center">
              <i class="fas fa-times text-red-600 text-2xl"></i>
            </div>
          </div>
          <div class="flex-1">
            <div class="bg-white rounded-lg p-6 shadow-sm">
              <div class="text-center">
                <div class="w-20 h-20 mx-auto bg-red-100 rounded-full flex items-center justify-center mb-4">
                  <i class="fas fa-paint-brush text-red-600 text-2xl"></i>
                </div>
                <h4 class="font-semibold text-gray-800 mb-2">Generation</h4>
                <p class="text-sm text-gray-600">❌ Fails to generate "yellow broccoli"</p>
              </div>
            </div>
          </div>
        </div>
        <div class="mt-6 text-center">
          <p class="text-gray-700">
            <span class="font-medium">The Gap:</span> UMMs often understand concepts they cannot generate, revealing fundamental misalignment between understanding and generation pathways.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Approach Section -->
  <section id="approach" class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">RecA: Semantic-Level Image Reconstruction</h2>
        <div class="mt-4 h-1 w-20 bg-primary-600 mx-auto"></div>
      </div>
      
      <!-- Key Training Advantage -->
      <div class="bg-gradient-to-r from-green-50 to-blue-50 rounded-xl shadow-md p-8 max-w-6xl mx-auto mb-12">
        <div class="text-center">
          <div class="flex justify-center items-center mb-4">
            <div class="w-12 h-12 bg-green-100 rounded-full flex items-center justify-center mr-4">
              <i class="fas fa-check-circle text-green-600 text-2xl"></i>
            </div>
            <h3 class="text-2xl font-semibold text-gray-800">Training Without Text-to-Image Data</h3>
          </div>
          <p class="text-lg text-gray-700 max-w-4xl mx-auto">
            <span class="font-bold text-green-600">RecA requires NO text-to-image paired data for training.</span> 
            Instead, we use pure <span class="font-medium text-blue-600">image-to-image reconstruction</span> with understanding visual embeddings.
          </p>
        </div>
      </div>

      <!-- Pipeline Overview -->
      <div class="bg-white rounded-xl shadow-md p-8 max-w-6xl mx-auto">
        <div class="mb-8">
          <img src="static/files/teaser/pipeline.jpg" alt="RecA Pipeline Overview" class="w-full h-auto rounded-lg shadow-sm">
        </div>
        
        <div class="prose-lg text-gray-700">
          <p class="mb-6">
            <span class="font-semibold text-gray-800">Overview of the semantic reconstruction realignment (RecA) pipeline.</span>
            A visual <span class="italic">understanding</span> encoder (<span class="italic">e.g.</span>, CLIP or DINO) extracts semantic features from the input image, which are fused with template text embeddings and passed to a Unified Multimodal Model (UMM) to regenerate the image.
          </p>
          <p class="mb-6">
            RecA implements a <span class="font-medium text-primary-600">self-supervised training paradigm</span> where the UMM is optimized with a reconstruction loss (diffusion or cross-entropy) between the original and reconstructed images. This approach provides dense supervision that preserves almost all fine-grained details that captions omit.
          </p>
          
          <!-- Key Benefits Grid -->
          <div class="grid md:grid-cols-2 gap-6 mt-8">
            <div class="bg-gray-50 rounded-lg p-6">
              <div class="flex items-start">
                <div class="w-8 h-8 bg-blue-100 rounded-full flex items-center justify-center mr-3 mt-1">
                  <i class="fas fa-image text-blue-600"></i>
                </div>
                <div>
                  <h4 class="font-semibold text-gray-800 mb-2">Self-Supervised Training</h4>
                  <p class="text-sm text-gray-600">Pure image reconstruction without relying on text captions or paired data.</p>
                </div>
              </div>
            </div>
            <div class="bg-gray-50 rounded-lg p-6">
              <div class="flex items-start">
                <div class="w-8 h-8 bg-green-100 rounded-full flex items-center justify-center mr-3 mt-1">
                  <i class="fas fa-brain text-green-600"></i>
                </div>
                <div>
                  <h4 class="font-semibold text-gray-800 mb-2">Understanding Visual Embeddings</h4>
                  <p class="text-sm text-gray-600">Dense visual features are extracted from understanding encoders (CLIP, SigLIP, etc.) as "visual prompts", not from generation encoders (VAE). </p>
                </div>
              </div>
            </div>
          </div>
          
          <div class="bg-blue-50 border-l-4 border-blue-400 p-6 rounded-r-lg mt-8">
            <p class="text-blue-800 font-medium">
              <span class="font-bold"><em>At inference time, RecA requires no additional inputs beyond the text prompt, operating as a standard UMM.</em></span>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section id="results" class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">State-of-the-Art Performance</h2>
        <div class="mt-4 h-1 w-20 bg-primary-600 mx-auto"></div>
      </div>
      
      <div class="bg-gray-50 rounded-xl shadow-md p-8 max-w-5xl mx-auto">
        <div class="prose-lg text-gray-700">
          <p class="mb-6">
            After only a few training steps, all models post large <span class="font-bold text-primary-600">zero-shot</span> gains in generation capability with <span class="italic">no loss in vision-understanding accuracy</span>. Our fine-tuned Harmon model, even with just 1.5B parameters, achieves a high score of <span class="font-bold text-primary-600">0.86</span> on GenEval and <span class="font-bold text-primary-600">87.21</span> on DPGBench, significantly outperforming the previous state-of-the-art models <span class="font-bold">without any GPT-4o-Image distillation data or reinforcement learning</span>.
          </p>
          <p>
            The most effective approach is a <span class="italic">two-stage strategy</span>: first applying SFT followed by reconstruction tuning, which achieves <span class="font-bold text-primary-600">0.90</span> on GenEval and <span class="font-bold text-primary-600">88.15</span> on DPGBench.
          </p>
        </div>
        
        <div class="mt-10">
          <img src="static/files/teaser/main.jpg" alt="Table 1: Benchmark Comparison" class="w-full h-auto rounded-lg shadow-sm">
        </div>
      </div>
    </div>
  </section>

  <!-- Generalizability Section -->
  <section class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">Enhanced Generalizability</h2>
        <p class="mt-4 text-lg text-gray-600 max-w-3xl mx-auto">
          Across Different Architectures and Tasks
        </p>
      </div>
      
      <div class="bg-white rounded-xl shadow-md p-8 max-w-5xl mx-auto">
        <div class="prose-lg text-gray-700">
          <p class="mb-6">
            RecA achieves consistent performance gains across different UMM frameworks, showcasing its generalizability. We apply RecA to various unified multimodal models including Show-o (AR), Harmon (AR+MAR), OpenUni (AR+Diffusion), and BAGEL (AR+Diffusion).
          </p>
          <p class="mb-6">
            All models demonstrate significant improvements through RecA: the most notable improvement is achieved by Harmon-1.5B with 85.7 GenEval score (+12.8). Our method exhibits the most significant gains in <span class="font-medium">Position</span> and <span class="font-medium">Color Attribution</span> tasks, while maintaining correct subjects, bindings, and positions across cases with <span class="italic">multiple objects</span>, <span class="italic">complex attributions</span>, and explicit <span class="italic">spatial layouts</span>.
          </p>
        </div>
        
        <div class="mt-10">
          <img src="static/files/teaser/t2i_result.jpg" alt="Text-to-Image Generation Results" class="w-full h-auto rounded-lg shadow-sm">
        </div>
      </div>
    </div>
  </section>

  <!-- Enhanced Editing Section -->
  <section class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="text-center mb-12">
        <h2 class="text-3xl font-bold text-gray-900">Enhanced Editing Capabilities</h2>
        <div class="mt-4 h-1 w-20 bg-primary-600 mx-auto"></div>
      </div>
      
      <div class="bg-gray-50 rounded-xl shadow-md p-8 max-w-5xl mx-auto">
        <div class="prose-lg text-gray-700">
          <p class="mb-6">
            We surprisingly discover that, for models with <span class="italic">image editing capabilities</span>, our method also significantly improves their editing performance. RecA demonstrates consistent improvements across all editing categories, increasing the ImgEdit scores from 3.38 to <span class="font-bold text-primary-600">3.75</span> and GEdit from 6.94 to <span class="font-bold text-primary-600">7.25</span>, using only <span class="italic">1,000 training steps and 8,000 unlabeled images</span>.
          </p>
          <p>
            Our method unlocks the model's inherent editing potential without expensive annotation across various tasks like addition, replacement, stylization and color modification.
          </p>
        </div>
        
        <div class="mt-10">
          <img src="static/files/teaser/edit_result.jpg" alt="Image Editing Results" class="w-full h-auto rounded-lg shadow-sm">
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="bg-gray-900 text-white py-12">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="flex flex-col md:flex-row justify-between items-center">
        <div class="flex items-center mb-6 md:mb-0">
          <img src="static/files/teaser/logo.png" alt="RecA Logo" class="h-10 w-10 mr-3">
          <span class="text-xl font-semibold">RecA</span>
        </div>
        
        <div class="flex space-x-6">
          <a href="https://arxiv.org/pdf/2412.17910" target="_blank" class="text-gray-300 hover:text-white transition">
            <i class="fas fa-file-pdf text-xl"></i>
          </a>
          <a href="https://arxiv.org/abs/2412.17910" target="_blank" class="text-gray-300 hover:text-white transition">
            <i class="fas fa-book-open text-xl"></i>
          </a>
          <a href="https://github.com/HorizonWind2004/RecA" target="_blank" class="text-gray-300 hover:text-white transition">
            <i class="fab fa-github text-xl"></i>
          </a>
          <a href="https://huggingface.co/collections/sanaka87/reca-68ad2176380355a3dcedc068" target="_blank" class="text-gray-300 hover:text-white transition">
            <i class="fas fa-robot text-xl"></i>
          </a>
        </div>
      </div>
      
      <div class="mt-8 pt-8 border-t border-gray-800 text-center text-gray-400 text-sm">
        <p>© 2024 RecA Research Team. All rights reserved.</p>
      </div>
    </div>
  </footer>

  <!-- Back to top button -->
  <button id="back-to-top" class="fixed bottom-8 right-8 bg-primary-600 text-white p-3 rounded-full shadow-lg opacity-0 invisible transition-all duration-300">
    <i class="fas fa-arrow-up"></i>
  </button>

  <script>
    // Back to top button
    const backToTopButton = document.getElementById('back-to-top');
    
    window.addEventListener('scroll', () => {
      if (window.pageYOffset > 300) {
        backToTopButton.classList.remove('opacity-0', 'invisible');
        backToTopButton.classList.add('opacity-100', 'visible');
      } else {
        backToTopButton.classList.remove('opacity-100', 'visible');
        backToTopButton.classList.add('opacity-0', 'invisible');
      }
    });
    
    backToTopButton.addEventListener('click', () => {
      window.scrollTo({
        top: 0,
        behavior: 'smooth'
      });
    });
    
    // Smooth scrolling for anchor links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        
        document.querySelector(this.getAttribute('href')).scrollIntoView({
          behavior: 'smooth'
        });
      });
    });
    
    // Demo gallery functionality
    document.addEventListener('DOMContentLoaded', function() {
      const demoItems = document.querySelectorAll('.demo-item');
      const promptDisplay = document.getElementById('promptDisplay');
      
      // Handle demo item clicks
      demoItems.forEach(item => {
        item.addEventListener('click', function() {
          const prompt = this.getAttribute('data-prompt');
          
          // Remove active class from all items
          demoItems.forEach(i => i.classList.remove('active'));
          
          // Add active class to clicked item
          this.classList.add('active');
          
          // Update prompt display
          promptDisplay.innerHTML = `<div class="prompt-text"><strong>Prompt:</strong> ${prompt}</div>`;
          promptDisplay.classList.add('active');
        });
      });
      
      // Edit comparison slider functionality
      const editContainers = document.querySelectorAll('.edit-comparison-container');
      
      editContainers.forEach(container => {
        const slider = container.querySelector('.edit-slider');
        const editResult = container.querySelector('.edit-result');
        const containerRect = container.getBoundingClientRect();
        let isDragging = false;
        let animationFrame = null;
        
        function updateSliderPosition(clientX) {
          if (animationFrame) {
            cancelAnimationFrame(animationFrame);
          }
          
          animationFrame = requestAnimationFrame(() => {
            const rect = container.getBoundingClientRect();
            const x = Math.max(0, Math.min(clientX - rect.left, rect.width));
            const percentage = (x / rect.width) * 100;
            
            // Update slider position
            slider.style.left = `${percentage}%`;
            
            // Update clip-path for the edited image
            editResult.style.clipPath = `polygon(${percentage}% 0%, 100% 0%, 100% 100%, ${percentage}% 100%)`;
          });
        }
        
        // Mouse events
        slider.addEventListener('mousedown', (e) => {
          isDragging = true;
          e.preventDefault();
        });
        
        document.addEventListener('mousemove', (e) => {
          if (isDragging) {
            updateSliderPosition(e.clientX);
          }
        });
        
        document.addEventListener('mouseup', () => {
          isDragging = false;
        });
        
        // Touch events for mobile
        slider.addEventListener('touchstart', (e) => {
          isDragging = true;
          e.preventDefault();
        });
        
        document.addEventListener('touchmove', (e) => {
          if (isDragging && e.touches[0]) {
            updateSliderPosition(e.touches[0].clientX);
            e.preventDefault(); // Prevent scrolling
          }
        });
        
        document.addEventListener('touchend', () => {
          isDragging = false;
        });
        
        // Click on container to move slider
        container.addEventListener('click', (e) => {
          if (!isDragging && e.target !== slider) {
            updateSliderPosition(e.clientX);
          }
        });
      });
    });
  </script>
</body>
</html>